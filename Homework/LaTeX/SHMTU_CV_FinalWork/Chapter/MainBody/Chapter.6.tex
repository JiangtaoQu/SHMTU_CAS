\chapter{模型量化的探究}
\label{chapter:6}

\section{介绍}

NVIDIA于2018年推出了基于Turing架构的革新性GPU，其中最引人注目的亮点莫过于全新的Tensor Core技术。这一技术利用了专门的ASIC（应用特定集成电路）来加速FP16（半精度浮点数）矩阵运算，为深度学习领域带来了显著的性能提升。FP16的位宽仅为FP32（单精度浮点数）的一半，虽然在一定程度上牺牲了数值精度，但实际应用中模型的准确度并未受到严重影响。因此，通过采用半精度量化技术，不仅可以大幅提升模型的推理速度，还能有效降低模型所需的存储空间。更为灵活的是，Turing架构还支持混合精度训练，允许在某些层使用半精度进行加速，同时保持其他层的单精度以确保准确性。

然而，我当前使用的是基于Pascal架构的NVIDIA Tesla P40 GPU。与Turing架构相比，Pascal架构并未配备专门的Tensor Core单元，因此在加速半精度计算方面存在一定局限。正因如此，在我的深度学习训练中，我主要依赖于纯单精度（FP32）进行计算。尽管如此，Pascal架构依然以其卓越的性能和稳定性在深度学习领域占有一席之地，为众多研究者和开发者提供了强大的计算支持。

\section{数据类型介绍}

\subsection{FP32}

FP32，即单精度浮点数（Single-Precision Floating-Point），是一种计算机中用于表示浮点数的数据类型。FP32使用32位（即4字节）来存储一个浮点数，其中通常包括1位符号位、8位指数位和23位尾数位（有效数字位）。

FP32的特点如下：

\begin{enumerate}
	\item 动态范围：由于有8位用于指数，FP32可以表示非常大或非常小的数，动态范围较广。
	\item 精度：23位的尾数提供了相对较高的精度，适用于需要精确计算的科学和工程应用。
	\item 通用性：FP32是大多数计算机系统和编程语言的默认浮点数表示，具有广泛的软件和硬件支持。
	\item 性能：虽然在现代硬件上，针对半精度（FP16）或混合精度（如TF32、BF16等）的优化可能提供更高的性能，但FP32仍然是许多高性能计算（HPC）和深度学习应用中不可或缺的格式，特别是在需要高精度的场景下。
	\item 稳定性：在某些复杂的数值计算中，使用FP32可以减少数值不稳定性和舍入误差。
\end{enumerate}

在深度学习中，FP32一度是训练和部署模型的标准格式，因为它提供了足够的精度来确保模型的正确性和稳定性。然而，随着模型规模的增大和对计算效率要求的提高，人们开始探索使用更低精度的格式（如FP16、BF16、INT8等）来加速训练和推理过程，同时尽量保持模型的性能。尽管有这些低精度格式的挑战者，FP32仍然在许多需要高精度或复杂数值处理的深度学习应用中占据重要地位。

\subsection{FP64}

FP64，即双精度浮点数（Double-Precision Floating-Point），是一种计算机中用于表示浮点数的数据类型，它使用64位（即8字节）来存储一个浮点数。这64位通常被分配为1位符号位、11位指数位和52位尾数位（有效数字位）。

FP64的特点如下：

\begin{enumerate}
	\item 高精度：由于有52位的尾数，FP64提供了非常高的数值精度，适用于需要极高精确度的科学和工程计算，如天文学、物理学模拟、金融建模等。
	\item 大动态范围：11位的指数位允许FP64表示极大和极小的数值，这使得它能够处理广泛的数值范围，而不会遇到溢出或下溢的问题。
	\item 稳定性：在涉及大量计算和复杂数学运算的应用中，使用FP64可以减少舍入误差和数值不稳定性，从而得到更可靠和准确的结果。
	\item 广泛的软件支持：几乎所有的编程语言和计算系统都支持FP64数据类型，这保证了其在各种应用中的通用性和兼容性。
	\item 性能考量：尽管FP64提供了高精度和高稳定性，但它也带来了更高的计算和存储成本。相比于FP32或更低精度的格式，FP64的计算速度通常较慢，且占用的内存和存储空间更大。因此，在性能敏感的应用中，如实时系统、嵌入式设备或大规模数据分析中，可能会优先考虑使用更低精度的数据类型。
\end{enumerate}

在深度学习中，FP64通常不是首选的数据类型，因为深度学习模型往往可以容忍一定程度的数值误差，并且更关注于计算效率和内存使用。然而，在某些特定场景下，如需要极高精度的模型训练、复杂的数值分析或与其他高精度科学计算软件的交互中，FP64可能是必要的。

\subsection{FP16}

FP16量化，即半精度浮点数量化，是一种在深度学习和机器学习领域中常用的优化技术。FP16指的是使用16位（bit）来表示浮点数，相较于传统的32位浮点数（FP32，即单精度浮点数），它减少了一半的存储空间和内存带宽需求。

在深度学习模型中，权重、激活值和梯度等参数通常都是以浮点数形式存储和计算的。传统的FP32提供了较高的数值精度和动态范围，但在很多深度学习应用中，并不需要这么高的精度。实际上，使用FP16往往可以在保证模型准确性的同时，显著提升计算性能和存储效率。

FP16量化的主要优势包括：

\begin{enumerate}
	\item 性能提升：半精度计算可以显著减少内存访问和传输的开销，因为数据的大小减少了一半。这对于需要大量数据传输的GPU加速计算尤为重要。
	\item 存储节省：模型的大小减半，这对于部署到存储空间有限的设备上（如移动设备、嵌入式系统）非常有利。
	\item 功耗降低：更少的内存访问和数据传输意味着更低的功耗，这对于电池供电的设备尤为重要。
	\item 硬件加速：一些现代GPU和AI加速器提供了对FP16计算的专门优化，如NVIDIA的Tensor Cores，这些硬件可以显著加速半精度计算。
\end{enumerate}

然而，使用FP16量化也需要注意一些问题：

\begin{enumerate}
	\item 数值稳定性：由于FP16的精度较低，可能会导致梯度消失或爆炸，从而影响模型的训练稳定性。因此，可能需要使用混合精度训练（Mixed Precision Training）等技术来平衡性能和稳定性。
	\item 动态范围：FP16的动态范围比FP32小，可能无法表示非常大或非常小的数值。这可能需要通过适当的缩放或归一化技术来处理。
	\item 软件支持：虽然许多深度学习框架和库都支持FP16计算，但仍然需要确保使用的软件和工具链完全兼容并能够正确处理半精度数值。
\end{enumerate}

总的来说，FP16量化是一种有效的深度学习优化技术，可以在保证模型性能的同时显著提高计算效率和存储效率。

\subsection{INT8}

INT8量化是一种模型优化技术，旨在将深度学习模型中的权重和激活值从原始的32位浮点数（FP32）格式转换为8位整数（INT8）格式，以减小模型大小、降低功耗并加快计算速度。

INT8量化的主要优势包括：
\begin{enumerate}
	\item 减小模型大小：通过使用8位整数代替32位浮点数，可以显著减小模型的大小，这对于部署到资源受限的设备（如移动设备或嵌入式系统）上非常有利。
	\item 加快计算速度：在许多硬件平台上，整数运算比浮点运算更快，因此INT8量化可以加速模型的推理速度。
	\item 降低功耗：整数运算通常比浮点运算更节能，这对于电池供电的设备尤为重要。
	然而，INT8量化也面临一些挑战：
\end{enumerate}

\begin{enumerate}
	\item 精度损失：由于从FP32转换到INT8会损失一些精度，这可能会影响模型的性能。因此，在量化过程中需要采取一些策略来最小化精度损失，例如使用校准数据集来调整量化参数。
	\item 硬件和软件支持：为了充分利用INT8量化的优势，需要硬件和软件的支持。一些现代的处理器和加速器提供了对INT8运算的优化，而深度学习框架和库也需要提供对INT8量化的支持。
\end{enumerate}

INT8量化的过程通常包括以下几个步骤：

\begin{enumerate}
	\item 模型训练：首先，使用FP32格式训练深度学习模型。
	\item 校准数据集：选择一个代表性的数据集作为校准数据集，用于调整量化参数。
	\item 量化参数计算：使用校准数据集计算量化参数（如缩放因子和偏移量），这些参数将用于将FP32值映射到INT8范围。
	\item 模型量化：使用计算得到的量化参数将模型的权重和激活值从FP32转换为INT8。
	\item 模型验证：验证量化后的模型在性能上是否与原始模型相似。如果需要，可以对量化后的模型进行微调以恢复性能。
\end{enumerate}

总的来说，INT8量化是一种有效的深度学习模型优化技术，可以显著减小模型大小、加快计算速度并降低功耗。然而，为了充分利用其优势，需要仔细选择校准数据集、计算量化参数，并确保硬件和软件的支持。

\subsection{BF32}

BF32（Bfloat16） 是一种浮点数表示格式，它使用16位（2字节）来存储数据，但具有与常规的16位浮点数（即FP16）不同的位布局和指数范围。BFloat16格式特别设计用于深度学习和其他需要高精度但不需要全范围32位浮点数的应用。

在BFloat16格式中，1位用于符号（s），8位用于指数（e），而7位用于尾数（m，也称为有效数字或分数）。这种布局提供了比标准FP16更大的指数范围，但牺牲了尾数的精度。这种权衡使得BFloat16在处理深度学习中的大范围数值时更为有效，同时减少了存储和计算需求。

BF32的优势主要包括：

\begin{enumerate}
	\item 减少存储需求：与FP32相比，BF32将存储需求减半，这对于大规模模型和数据集来说非常重要。
	\item 加速计算：许多现代硬件平台（如GPU和TPU）都针对BF32优化，从而可以加速深度学习训练和推理。
	\item 降低功耗：减少存储和计算需求通常也意味着更低的功耗，这对于数据中心和边缘设备都是重要的考虑因素。
\end{enumerate}

保持足够的精度：尽管BF32的尾数精度较低，但在许多深度学习场景中，它仍能提供足够的精度来维持模型性能。

需要注意的是，虽然BF32在深度学习领域很有用，但它并不总是适用于所有类型的计算或所有模型。在某些情况下，使用FP32或FP16可能更为合适，具体取决于模型的复杂性、所需的精度以及硬件支持等因素。因此，在选择数值表示格式时，需要根据具体的应用场景和需求进行权衡。

\subsection{TF32}

TF32（TensorFloat-32） 是 NVIDIA 为其 Ampere 架构的 GPU 引入的一种新的数学模式，专为深度学习而设计。TF32 旨在结合 FP16（半精度浮点数）和 FP32（单精度浮点数）的优势，以在深度学习训练和推理中实现更高的吞吐量和更快的速度，同时保持与 FP32 相当的精度。

TF32 的工作原理是在内部使用 19 位尾数（mantissa）和 10 位指数（exponent）来表示浮点数，这与标准的 FP32 和 FP16 格式不同。这种表示法允许 TF32 在处理深度学习中的大范围数值时保持较高的精度，同时减少了存储和计算需求。然而，需要注意的是，TF32 并不是一种标准的 IEEE 浮点数格式，而是 NVIDIA 专为其 GPU 平台定制的一种格式。

使用 TF32 的主要优势包括：

\begin{enumerate}
	\item 性能提升：TF32 模式允许 GPU 在执行深度学习操作时实现更高的吞吐量和更快的速度。这主要归功于 TF32 在内部处理数据时所使用的优化算法和硬件加速。
	\item 精度保持：尽管 TF32 使用了较少的位数来表示浮点数，但 NVIDIA 声称它在许多深度学习应用中能够提供与 FP32 相当的精度。这使得研究人员和开发人员能够在不牺牲模型性能的情况下加快训练和推理速度。
	\item 硬件支持：TF32 是 NVIDIA Ampere 架构 GPU 的一项特性，因此只有支持这一架构的 GPU 才能利用 TF32 的优势。对于拥有兼容硬件的用户来说，使用 TF32 可以是一种无需修改代码即可提升性能的有效方法。
	\item 软件兼容性：NVIDIA 的深度学习框架和库（如 TensorFlow、PyTorch 等）通常都支持 TF32。这意味着开发人员可以在不修改现有代码的情况下利用 TF32 的优势，只需确保他们的 GPU 和软件环境支持这一特性即可。
\end{enumerate}

然而，需要注意的是，尽管 TF32 在许多情况下都能提供显著的性能提升和精度保持，但它并不总是适用于所有类型的深度学习模型和应用。在某些特定场景下，使用 FP32 或其他数值表示格式可能更为合适。因此，在选择使用哪种数值表示格式时，需要根据具体的应用场景和需求进行权衡和测试。

\section{ONNX量化步骤}

\subsection{半精度FP16量化}

具体文件请参考\url{Train/SHMTU_CAS_OCR_RESNET/src/classify/digit/quantize_onnx_fp16.py}。

\begin{lstlisting}[caption={ONNX FP16量化},language=Python,label=code:onnx_fp16]
	import warnings

	import onnx
	from onnxconverter_common import float16

	model = onnx.load("resnet34_digit_latest.onnx")

	with warnings.catch_warnings():
	warnings.filterwarnings(
	"ignore", category=UserWarning,
	message="the float32 number .* will be truncated to .*"
	)
	model_fp16 = float16.convert_float_to_float16(model)

	onnx.save(model_fp16, "resnet34_digit_latest_fp16.onnx")

\end{lstlisting}

该Python代码片段(\ref{code:onnx_fp16})执行了以下步骤：

\begin{enumerate}
	\item {
		导入必要的库和模块：

		\begin{itemize}
			\item \texttt{warnings}：用于处理警告信息。
			\item \texttt{onnx}：用于加载和保存ONNX模型。
			\item \texttt{onnxconverter\_common.float16}：提供了将ONNX模型从float32转换为float16的功能。
		\end{itemize}
	}

	\item {加载模型

		加载预训练的ONNX模型（"resnet34\_digit\_latest.onnx"）到变量\texttt{model}。
	}

	\item {
		使用\texttt{warnings.catch\_warnings()}上下文管理器来忽略特定类型的警告信息。

		通过\texttt{warnings.filterwarnings()}函数，忽略了与float32到float16转换相关的用户警告。
	}

	\item {
		模型量化

		调用\texttt{float16.convert\_float\_to\_float16()}函数将\texttt{model}从float32格式转换为float16格式，并将转换后的模型存储在变量\texttt{model\_fp16}中。
	}

	\item {
		使用\texttt{onnx.save()}函数保存
	}

\end{enumerate}

此代码的目的是减小模型文件的大小并可能提高推理速度，但可能会以牺牲一些模型精度为代价。

\subsection{INT8动态量化}

具体文件请参考\url{Train/SHMTU_CAS_OCR_RESNET/src/classify/digit/quantize_onnx_int8.py}。

\begin{lstlisting}[caption={ONNX INT8量化},language=Python,label=code:onnx_int8]
	import onnx
	from onnxruntime.quantization import preprocess
	from onnxruntime.quantization import quantize_dynamic, QuantType

	from pathlib import Path

	model_fp32 = "resnet34_digit_latest_p.onnx"
	model_quant = "resnet34_digit_latest_int8.onnx"

	model_fp32 = Path(model_fp32)
	model_quant = Path(model_quant)

	# python -m onnxruntime.quantization.preprocess --input resnet34_digit_latest.onnx --output resnet34_digit_latest_p.onnx

	quantize_dynamic(
	model_fp32,
	model_quant,
	weight_type=QuantType.QUInt8,
	)

\end{lstlisting}

该Python代码片段(\ref{code:onnx_int8})执行了以下步骤：

\begin{enumerate}
	\item 导入必要的库：
	\begin{enumerate}
		\item \texttt{onnx}: 用于加载和保存ONNX模型。
		\item \texttt{onnxruntime.quantization.preprocess}: 用于预处理模型，准备进行量化。
		\item \texttt{onnxruntime.quantization.quantize\_dynamic} 和 \texttt{QuantType}: 用于执行动态量化。
		\item \texttt{pathlib.Path}: 用于处理文件路径。
	\end{enumerate}
	\item 定义输入和输出模型的路径：
	\begin{enumerate}
		\item \texttt{model\_fp32}: 指向原始的32位浮点数模型文件。
		\item \texttt{model\_quant}: 指向将要保存的量化后的8位整数模型文件。
	\end{enumerate}
	\item 使用 \texttt{Path} 类将字符串路径转换为 \texttt{Path} 对象，以便进行更方便的路径操作。
	\item 注释行（以`\#`开头）指示了如何使用命令行工具对模型进行FP16量化处理，这是官方文档中推荐的做法，首先进行FP16量化后再进行INT8量化。
	\item 调用 \texttt{quantize\_dynamic} 函数对模型进行动态量化：
	\begin{enumerate}
		\item 输入模型：\texttt{model\_fp32}
		\item 输出模型：\texttt{model\_quant}
		\item 权重类型：\texttt{QuantType.QUInt8}，表示使用无符号8位整数进行量化。
	\end{enumerate}
\end{enumerate}

量化后的模型可用于减少模型大小和提高推理速度，同时保持模型的准确性。这对于在资源受限的设备上部署模型特别有用。

这里使用的是动态INT8的方法进行模型的量化，下面在NCNN中将介绍静态量化的方法。

\section{NCNN量化步骤}

我们使用官方提供的转换工具进行操作。

由于ONNX使用Google Protobuf进行序列化，然而Linux操作系统中的Protobuf或通过包管理器(如APT)安装的Protobuf有很大概率与官方Github Release提供的预编译版本所需的Protobuf版本不同，因此我们选择兼容性极好的Windows来执行NCNN的量化操作。

\subsection{半精度FP16量化}

NCNN FP16量化脚本(\ref{code:ncnn_fp16})请参考\url{Train/SHMTU_CAS_OCR_RESNET/convert_onnx_to_ncnn.ps1}。

\begin{lstlisting}[caption={NCNN FP16量化},label=code:ncnn_fp16]
	# Convert ONNX model to NCNN format
	# Usage: powershell -File convert_onnx_to_ncnn.ps1

	function ConvertModelToNCNN {
		param (
		[string]$modelName
		)

		$toolPath = ".\3rdparty\ncnn\bin"
		$pthDirPath = ".\workdir\Models"

		Write-Host "Converting $modelName to NCNN format"

		& "$toolPath\onnx2ncnn.exe" `
		"$pthDirPath\$modelName.onnx" `
		"$pthDirPath\$modelName.fp32.param" `
		"$pthDirPath\$modelName.fp32.bin"

		# Optimize quantization, 1: fp16, 0: fp32
		& "$toolPath\ncnnoptimize.exe" `
		"$pthDirPath\$modelName.fp32.param" `
		"$pthDirPath\$modelName.fp32.bin" `
		"$pthDirPath\$modelName.fp16.param" `
		"$pthDirPath\$modelName.fp16.bin" `
		1
	}

	ConvertModelToNCNN -modelName "resnet18_equal_symbol_latest"
	ConvertModelToNCNN -modelName "resnet18_operator_latest"
	ConvertModelToNCNN -modelName "resnet34_digit_latest"

\end{lstlisting}

ncnnoptimize工具能够自动将FP32模型量化为FP16模型，而且精度损失不会太大。

\subsection{INT8静态量化}

\begin{enumerate}
	\item 将ONNX转换为NCNN格式
	\item 进行FP16量化(可选)
	\item 准备一组数据用于校准
	\item 使用ncnn2table生成校准表
	\item 使用ncnn2int8进行INT8量化
\end{enumerate}

\subsubsection{生成校准表}

\begin{lstlisting}[caption={生成校准表},label=code:ncnn_int8_table]
	./ncnn2table `
		r34_digit.param `
		r34_digit.bin image_digit_list.txt `
		r34_digit.table `
		mean=[123.675,116.280,103.530] `
		norm=[0.017,0.018,0.017] `
		shape=[1,3,224,224] `
		pixel=RGB `
		thread=8 `
		method=kl
\end{lstlisting}

具体命令行请参考代码\ref{code:ncnn_int8_table}

\begin{enumerate}
	\item 使用"find image\_digit/ -type f > image\_digit\_list.txt"将图片文件名生成txt格式列表
	\item 使用ncnn2table工具生成校准表
\end{enumerate}

\subsubsection{静态量化}

将FP32模型或FP16模型，以及校准表传入ncnn2int8工具即可进行INT8量化，具体命令行请参考代码\ref{code:ncnn_int8_q}。

\begin{lstlisting}[caption={NCNN INT8量化},label=code:ncnn_int8_q]
	./ncnn2int8 r34_digit.param r34_digit.bin r34_digit_8.param r34_digit_8.bin r34_digit.table
\end{lstlisting}

\section{实验}

\subsection{FP16}

经过Selenium的验证，FP16模型能够有90\%的正确率，因此适合进行部署。

\subsection{INT8}

经过试验，INT8量化阿拉伯数字识别模型后无法正确识别数字3，因此不适合使用INT8量化。
